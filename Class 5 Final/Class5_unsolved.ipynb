{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz6n5gTFq38H"
      },
      "source": [
        "# Dealing with Data Spring 2020 – Class 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKStz2feq38O"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8p1fJxhq38P"
      },
      "source": [
        "# Files and Printing\n",
        "\n",
        "Often you will need to read data from a file, or write the output of a Python script back to a file. \n",
        "\n",
        "We use the `open` function to open the file in the appropriate mode, which takes two arguments: \n",
        "\n",
        "1. the name of the file,\n",
        "2. and the mode. \n",
        "\n",
        "> `a_file = open(filename, mode)`\n",
        "\n",
        "The `mode` is a single letter string that specifies if you are going to be reading from a file, writing to a file, or appending to the end of an existing file. The modes are: \n",
        "\n",
        "+ `'r'` : open a file for reading\n",
        "+ `'w'` : open a file for writing (beware, this will overwrite any previously existing file) \n",
        "+ `'a'` : append (write to the end of a file) \n",
        "\n",
        "When reading a file, you usually want to iterate through the lines in that file using a `for loop`. Some other common methods for dealing with files are: \n",
        "\n",
        "+ `file.read()` : read the entire contents of a file into a string\n",
        "+ `file.write(some_string)` : writes to the file (note, this doesn't automatically include new lines) \n",
        "+ `file.close()` : close the open file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtBzRKFqq38Q"
      },
      "source": [
        "# Writing a file to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydpwOnH3q38Q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIQNJm6q38R"
      },
      "source": [
        "# Reading a file from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXzblL4Dq38S"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ajJ9AXgq38S"
      },
      "source": [
        "Once we read the file, we have the lines in a big string. Let's process that big string a little bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFI9raFIq38T"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^Please note that in Colab it takes a minute or so for the !rm changes to come into effect."
      ],
      "metadata": {
        "id": "_u6miFzx4XPM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy06siIeGRZM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "4GZojYnLW3O4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv9DsLKxUy5i"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFS_sVmPq38U"
      },
      "source": [
        "# Python `os` standard library\n",
        "Another addition to our file handling toolkit is the `os` library which provides ways to move files, make directories, and gather data about the file system. Like other standard libraries, we need to import it to use it via `import os`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LlzYGXtq38U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ Remember, all of the above is applying to our Colab environment. If you open up your computer's terminal or command line and type ```pwd``` (print working directory) you will most likely get a different response."
      ],
      "metadata": {
        "id": "0goJc63Z4wKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOylkvGZq38U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfV6HcaOwZNI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "RxbYfgUzwZNQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wteUtTXOwZNQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1-TP6qnq38W"
      },
      "source": [
        "# **Putting our Python to work: Exploring and Cleaning the Census Data file**\n",
        "\n",
        "Our approach will be as follows:\n",
        "* Inspect the data thoroughly to understand its benefits and risks for processing, and what information lies within\n",
        "* Clean/fix the data to remove any issues that would prevent it from working with SQL, such as weird characters, too many columns, missing data, splitting values into multiple rows, or combining multiple values into a single row\n",
        "* Structure the data found in the file as a Python native structure so we can manipulate and prepare it for use in SQL\n",
        "* Migrate our approach to a dedicated Python file\n",
        "\n",
        "We'll use: file read/write, loops, nested structures and UDFs to do all of this. (Some will be homework.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsjhNbbFq38X"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNswvX_q38X"
      },
      "source": [
        "## **1) Inspect our Data**\n",
        "\n",
        "Upon visual inspection:\n",
        "* there are 350+ columns\n",
        "* there are zipcodes for just NYC \n",
        "* there are a mix of letter and numbers for values\n",
        "* we have both percent and numbers values\n",
        "* it appears to be comma-separated\n",
        "* there are two column header lines: one with codes, and another with human-readable labels\n",
        "\n",
        "Next - we inspect with Python in two ways: first we'll look at the raw file as a string, then we'll probe whether the comma-separation is fail safe or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtY20TUNq38X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pulkvrwyq38X"
      },
      "source": [
        "To work in Google Colab, we must manually upload our CSV into the environment..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y-Q3iE3q38X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBcdJ4ANq38Y"
      },
      "source": [
        "# **Review Comma-Based Separability**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B39GABI4q38Y"
      },
      "source": [
        "In the first 500 characters, we see that the census has \"code labels\" for each column, none of which have funky characters that could trip us up with comma separation. \n",
        "\n",
        "We need to work with just that first line, though, so we'll use readline (which reads one entire line from the file) on the file handle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkNlGFxRq38Y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37KWx6hUq38Z"
      },
      "source": [
        "We knew it had 350+ and this is too many columns to effectively work with. \n",
        "\n",
        "Before we get to removing data, though, we need to put our data into a structure we can slice and dice. In other words, we need to transform our \"string\" data into a list of lists - a nested data structure where each row is a list, and within that list, each column is a list - a nested data structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVz_5ebqq38Z"
      },
      "source": [
        "For example: let's look at this simple 3x3 table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxXP04gFq38Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf26acnq38Z"
      },
      "source": [
        "## **2) Cleaning Up Wonky Data**\n",
        "\n",
        "But before we can even create a nested structure - we need to be confident we can split the data correctly for every single line.\n",
        "\n",
        "Unfortunately, CSV data is known to be particularly tricky because sometimes data sources use commas in column labels but surround those column headers with \"\" because Excel will treat it right. \n",
        "\n",
        "Python won't be so forgiving so we need to test for \"\" in the data - we'll do this using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SN8nX9wq38Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLaG063nq38Z"
      },
      "source": [
        "Uh-oh! those quotes spell trouble so now we need to see where that first quote appears and if a comma appears after it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAUTQxojq38Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5NbDHcq38Z"
      },
      "source": [
        "We suspect that those \"\"  in human-readable column headers of the second line of data are hiding \"...**,**...\" and will cause any nesting using comma-separation to create extra columns.\n",
        "\n",
        "Let's prove it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwqyyMTVq38a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsZsiK7q38a"
      },
      "source": [
        "Now that we've proven those will be a problem, we are going to use Python to clean those up via our own User Defined Function for this purpose, because it may appear in another data source, too.\n",
        "\n",
        "Before we create the UDF, let's describe what we want our function to do: \n",
        "\n",
        "1. it will accept a string input\n",
        "2. it will remove \" characters from the input\n",
        "3. when it finds a , between \"\" it will replace , characters in the input with a `-`. However, it will NOT change `,` otherwise since it is a CSV file.\n",
        "4. it will return a string output\n",
        "\n",
        "And, we will create a test_input and expected_output to test our UDF during development:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj5PP_Gmq38a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riaQfpmaq38a"
      },
      "source": [
        "It works! Now, let's fix our original input data string, `census_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpuaYEYsq38b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH-RvaJqq38b"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N7ya7Efq38b"
      },
      "source": [
        "## **3) Creating a Nested Structure**\n",
        "\n",
        "Now that we've cleaned up the wonkiness in the data, we can create our nested structure using a `for` loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp_e3TgMq38b"
      },
      "source": [
        "Let's start by reviewing our simple 3x3 table example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYbaRya3q38b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zdCVmxJq38b"
      },
      "source": [
        "In our example, the data is structured as:\n",
        "\n",
        "`list[row][column]`\n",
        "\n",
        "And we will use split commands to do the same for our data, this time creating another UDF.\n",
        "\n",
        "Again - as before - let's state what it will do and create test data:\n",
        "1. the UDF will take a data string as input\n",
        "* it will create a list where each line in the data, identified using `\\n`, is an item\n",
        "* for each item in the first list `list[row]`, it will create a list of columns, using comma-separation (`,`) to identify each item\n",
        "* it would be nice for the UDF user to specify what character separates rows and columns, separately\n",
        "* the UDF will return the nested data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybZosbYhq38b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW-2pMrlq38c"
      },
      "source": [
        "Now we can create a nested structure of our census data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjF-wD7oq38c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw6tgo7_q38c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nYOEdbFq38c"
      },
      "source": [
        "## **4) Moving from Notebooks to \\*.py Files** \n",
        "\n",
        "But before you go to do that, we need to start moving the findings of our exploration into a dedicated python file for cleaning the Census data. \n",
        "\n",
        "We're making this migration because notebooks are great for exploring data, but as our files and project grow larger, it is simpler to run the Python files outside notebooks AND sometimes very large files can cause our notebooks to crash.\n",
        "\n",
        "Here's what that new Python file needs to do:\n",
        "\n",
        "1. read the census data file into a variable\n",
        "2. clean the data by removing the \"...,...\" problem using a UDF\n",
        "3. create a nested data structure\n",
        "4. remove unwanted columns\n",
        "5. create a new string from the nested structure\n",
        "6. write the file to disk\n",
        "\n",
        "We've already #1, #2, #3, and #6 together, so we'll isolate those below along with comments to do the other work before putting in its own file.\n",
        "\n",
        "And recall that our UDFs has to be put ahead of the main program to work because the Main Program needs to know what happens in those UDFs.\n",
        "\n",
        "Below is the exact code that will be put into its own file named `clean_census.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cED5Eujvq38c"
      },
      "source": [
        "```python\n",
        "################################################################\n",
        "##### UNQUOTABLE #############################################\n",
        "################################################################\n",
        "\n",
        "def unquotable(input_string): # removes pesky , within \"\" values before changing , to \\t in program\n",
        "    \n",
        "    # remove \" char by splitting the input string into a list using that char\n",
        "    tmp_list = input_string.split('\"')\n",
        " \n",
        "    # remove , char\n",
        "    for i in range(len(tmp_list)):\n",
        "        # skip items that start/end with commas as these aren't quoted items\n",
        "        if (tmp_list[i][0] == \",\") or (tmp_list[i][-1] == \",\"):\n",
        "            continue\n",
        "        # replace , with - in quoted items\n",
        "        else:\n",
        "            tmp_list[i] = tmp_list[i].replace(\",\",\"-\")\n",
        "\n",
        "    # rejoin items into a string\n",
        "    output = \",\".join(tmp_list)\n",
        "\n",
        "    # get rid of any \",,\" and \",,,\" as it will add extra columns\n",
        "    output = output.replace(\",,,\",\",\")\n",
        "    output = output.replace(\",,\",\",\")\n",
        "    \n",
        "    # return string\n",
        "    return output\n",
        "\n",
        "################################################################\n",
        "##### NESTER #############################################\n",
        "################################################################\n",
        "\n",
        "def nester(input_string,input_row_delim,input_col_delim): # take a dataset string where each row is separated by input_row_delim and each column is separate by input_col_delim to create a nested object of lists\n",
        "\n",
        "    # create a list item for each row in the file using the row delimiter\n",
        "    row_list = input_string.split(input_row_delim)\n",
        "\n",
        "    #output var\n",
        "    nested_data = []\n",
        "    \n",
        "    # created nested structure to store each column separately\n",
        "    # list of rows where each row is a list of columns)\n",
        "    for i in range(len(row_list)): \n",
        "        row = row_list[i]\n",
        "        col = row.split(input_col_delim)\n",
        "        nested_data.append(col)\n",
        "    \n",
        "    # return the nested structure\n",
        "    return nested_data\n",
        "\n",
        "\n",
        "################################################################\n",
        "##### MAIN PROGRAM #############################################\n",
        "################################################################\n",
        "\n",
        "# 1. read the census data file into a variable\n",
        "\n",
        "file_handle = open('raw_census_2010_copy.csv',\"r\")\n",
        "census_data = file_handle.read()\n",
        "file_handle.close()\n",
        "\n",
        "# 2. clean the data by removing the \"...,...\" problem using a UDF\n",
        "\n",
        "clean_census_data = unquotable(census_data)\n",
        "\n",
        "# 3. create a nested data structure with a UDF\n",
        "\n",
        "nested_census_data = nester(clean_census_data,\"\\n\",\",\")\n",
        "\n",
        "# 4. remove unwanted columns using a UDF\n",
        "\n",
        "# HOMEWORK\n",
        "\n",
        "# 5. create a new string from the nested structure using a UDF\n",
        "\n",
        "# HOMEWORK\n",
        "\n",
        "# 6. write the file to disk\n",
        "\n",
        "file_handle = open('raw_census_2010_copy.csv',\"r\")\n",
        "file_handle.write('final data var from step #5 goes here')\n",
        "file_handle.close()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7AL-D6pwbq3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ **QUESTIONS?**"
      ],
      "metadata": {
        "id": "GxZgKkhuwbq3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DpLBSOqwbq3"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Class5_unsolved.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}